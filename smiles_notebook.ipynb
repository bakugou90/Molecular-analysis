{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Smiles toxicity classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import keras \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 1\n",
    "df_train_smiles=pd.read_csv(\"smiles_train.csv\")\n",
    "df_test_smiles=pd.read_csv(\"smiles_test.csv\")\n",
    "df_train_labels=pd.read_csv(\"y_train.csv\")\n",
    "df_test_labels=pd.read_csv(\"y_test.csv\")\n",
    "\n",
    "\n",
    "smiles_train=df_train_smiles['smiles']\n",
    "smiles_test=df_test_smiles['smiles']\n",
    "y_train=df_train_labels['labels']\n",
    "y_test=df_test_labels['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://iwatobipen.wordpress.com/2017/01/22/encode-and-decode-smiles-strings/\n",
    "\n",
    "SMILES_CHARS = [' ',\n",
    "                  '#', '%', '(', ')', '+', '-', '.', '/',\n",
    "                  '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                  '=', '@',\n",
    "                  'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "                  'R', 'S', 'T', 'V', 'X', 'Z',\n",
    "                  '[', '\\\\', ']',\n",
    "                  'a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\n",
    "                  't', 'u',':','d','G','h','W','U','D','y','Y']\n",
    "smi2index = dict( (c,i) for i,c in enumerate( SMILES_CHARS ) )\n",
    "index2smi = dict( (i,c) for i,c in enumerate( SMILES_CHARS ) )\n",
    "def smiles_encoder( smiles, maxlen=1000 ):\n",
    "    smiles =smiles\n",
    "    X = np.zeros( ( maxlen, len( SMILES_CHARS ) ) )\n",
    "    for i, c in enumerate( smiles ):\n",
    "        X[i, smi2index[c] ] = 1\n",
    "    return X\n",
    " \n",
    "def smiles_decoder( X ):\n",
    "    smi = ''\n",
    "    X = X.argmax( axis=-1 )\n",
    "    for i in X:\n",
    "        smi += index2smi[ i ]\n",
    "    return smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[smiles_encoder(i) for i in smiles_train]\n",
    "X_train=np.array(X_train)\n",
    "\n",
    "X_test=[smiles_encoder(i) for i in smiles_test]\n",
    "X_test=np.array(X_test)\n",
    "\n",
    "X=[]\n",
    "\n",
    "for i in np.arange(0,X_train.shape[0]):\n",
    "    X.append(X_train[i])\n",
    "    \n",
    "for i in np.arange(0,X_test.shape[0]):\n",
    "    X.append(X_test[i])\n",
    "X=np.array(X)\n",
    "    \n",
    "y=[]    \n",
    "\n",
    "for i in np.arange(0,y_train.shape[0]):\n",
    "    y.append(y_train[i])\n",
    "    \n",
    "for i in np.arange(0,y_test.shape[0]):\n",
    "    y.append(y_test[i])\n",
    "    \n",
    "y=np.array(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_2 = keras.utils.to_categorical(y_train)\n",
    "y_test_2 = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# import necessary building blocks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D,  Flatten, Dense, Activation, Dropout,BatchNormalization, MaxPool1D\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    \"\"\"\n",
    "    Define your model architecture here.\n",
    "    Returns `Sequential` model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(input_shape=X_train[0].shape,padding=\"same\",kernel_size=3,filters=16))\n",
    "    model.add(MaxPool1D())\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=32))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=32))\n",
    "    model.add(MaxPool1D())\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=64))\n",
    "    model.add(MaxPool1D())\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 1000, 16)          3136      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 500, 16)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 500, 16)           64        \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 500, 32)           1568      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)   (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)   (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 250, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 250, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)   (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               2048256   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,063,362\n",
      "Trainable params: 2,063,074\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5334 samples, validate on 2628 samples\n",
      "Epoch 1/25\n",
      "('Learning rate:', 0.005)\n",
      "5334/5334 [==============================] - 36s 7ms/step - loss: 0.3759 - accuracy: 0.8750 - val_loss: 0.9979 - val_accuracy: 0.1195\n",
      "Epoch 2/25\n",
      "('Learning rate:', 0.0045)\n",
      "5334/5334 [==============================] - 37s 7ms/step - loss: 0.3379 - accuracy: 0.8783 - val_loss: 0.5331 - val_accuracy: 0.8870\n",
      "Epoch 3/25\n",
      "('Learning rate:', 0.00405)\n",
      "5334/5334 [==============================] - 38s 7ms/step - loss: 0.3299 - accuracy: 0.8783 - val_loss: 2.7755 - val_accuracy: 0.1195\n",
      "Epoch 4/25\n",
      "('Learning rate:', 0.003645)\n",
      "5334/5334 [==============================] - 41s 8ms/step - loss: 0.3138 - accuracy: 0.8796 - val_loss: 3.5350 - val_accuracy: 0.1202\n",
      "Epoch 5/25\n",
      "('Learning rate:', 0.0032805)\n",
      "5334/5334 [==============================] - 34s 6ms/step - loss: 0.3089 - accuracy: 0.8796 - val_loss: 0.3300 - val_accuracy: 0.8805\n",
      "Epoch 6/25\n",
      "('Learning rate:', 0.00295245)\n",
      "5334/5334 [==============================] - 36s 7ms/step - loss: 0.3027 - accuracy: 0.8793 - val_loss: 0.5958 - val_accuracy: 0.6712\n",
      "Epoch 7/25\n",
      "('Learning rate:', 0.002657205)\n",
      "5334/5334 [==============================] - 34s 6ms/step - loss: 0.2995 - accuracy: 0.8793 - val_loss: 0.4044 - val_accuracy: 0.8805\n",
      "Epoch 8/25\n",
      "('Learning rate:', 0.0023914846)\n",
      "5334/5334 [==============================] - 36s 7ms/step - loss: 0.2835 - accuracy: 0.8795 - val_loss: 0.3237 - val_accuracy: 0.8824\n",
      "Epoch 9/25\n",
      "('Learning rate:', 0.002152336)\n",
      "5334/5334 [==============================] - 38s 7ms/step - loss: 0.2750 - accuracy: 0.8817 - val_loss: 0.3386 - val_accuracy: 0.8805\n",
      "Epoch 10/25\n",
      "('Learning rate:', 0.0019371024)\n",
      "5334/5334 [==============================] - 36s 7ms/step - loss: 0.2765 - accuracy: 0.8825 - val_loss: 0.3220 - val_accuracy: 0.8801\n",
      "Epoch 11/25\n",
      "('Learning rate:', 0.0017433922)\n",
      "5334/5334 [==============================] - 42s 8ms/step - loss: 0.2720 - accuracy: 0.8819 - val_loss: 0.3204 - val_accuracy: 0.8801\n",
      "Epoch 12/25\n",
      "('Learning rate:', 0.0015690529)\n",
      "5334/5334 [==============================] - 41s 8ms/step - loss: 0.2634 - accuracy: 0.8828 - val_loss: 0.3441 - val_accuracy: 0.8801\n",
      "Epoch 13/25\n",
      "('Learning rate:', 0.0014121477)\n",
      "5334/5334 [==============================] - 33s 6ms/step - loss: 0.2577 - accuracy: 0.8909 - val_loss: 0.6346 - val_accuracy: 0.6682\n",
      "Epoch 14/25\n",
      "('Learning rate:', 0.001270933)\n",
      "5334/5334 [==============================] - 33s 6ms/step - loss: 0.2593 - accuracy: 0.8941 - val_loss: 0.3402 - val_accuracy: 0.8919\n",
      "Epoch 15/25\n",
      "('Learning rate:', 0.0011438397)\n",
      "5334/5334 [==============================] - 34s 6ms/step - loss: 0.2530 - accuracy: 0.8971 - val_loss: 0.5159 - val_accuracy: 0.7549\n",
      "Epoch 16/25\n",
      "('Learning rate:', 0.0010294557)\n",
      "5334/5334 [==============================] - 33s 6ms/step - loss: 0.2454 - accuracy: 0.9023 - val_loss: 0.3254 - val_accuracy: 0.8919\n",
      "Epoch 17/25\n",
      "('Learning rate:', 0.0009265101)\n",
      "5334/5334 [==============================] - 33s 6ms/step - loss: 0.2442 - accuracy: 0.9025 - val_loss: 0.3625 - val_accuracy: 0.8809\n",
      "Epoch 18/25\n",
      "('Learning rate:', 0.0008338591)\n",
      "5334/5334 [==============================] - 33s 6ms/step - loss: 0.2394 - accuracy: 0.9038 - val_loss: 0.3387 - val_accuracy: 0.8935\n",
      "Epoch 19/25\n",
      "('Learning rate:', 0.0007504732)\n",
      "5334/5334 [==============================] - 32s 6ms/step - loss: 0.2308 - accuracy: 0.9066 - val_loss: 0.3410 - val_accuracy: 0.8912\n",
      "Epoch 20/25\n",
      "('Learning rate:', 0.00067542586)\n",
      "5334/5334 [==============================] - 32s 6ms/step - loss: 0.2327 - accuracy: 0.9042 - val_loss: 0.3560 - val_accuracy: 0.8931\n",
      "Epoch 21/25\n",
      "('Learning rate:', 0.00060788327)\n",
      "5334/5334 [==============================] - 33s 6ms/step - loss: 0.2279 - accuracy: 0.9087 - val_loss: 0.3526 - val_accuracy: 0.8973\n",
      "Epoch 22/25\n",
      "('Learning rate:', 0.00054709497)\n",
      "5334/5334 [==============================] - 32s 6ms/step - loss: 0.2297 - accuracy: 0.9078 - val_loss: 0.3985 - val_accuracy: 0.8524\n",
      "Epoch 23/25\n",
      "('Learning rate:', 0.0004923855)\n",
      "5334/5334 [==============================] - 32s 6ms/step - loss: 0.2324 - accuracy: 0.9066 - val_loss: 0.4534 - val_accuracy: 0.8135\n",
      "Epoch 24/25\n",
      "('Learning rate:', 0.0004431469)\n",
      "5334/5334 [==============================] - 32s 6ms/step - loss: 0.2292 - accuracy: 0.9119 - val_loss: 0.3463 - val_accuracy: 0.8866\n",
      "Epoch 25/25\n",
      "('Learning rate:', 0.00039883223)\n",
      "5334/5334 [==============================] - 33s 6ms/step - loss: 0.2248 - accuracy: 0.9085 - val_loss: 0.3377 - val_accuracy: 0.8995\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "INIT_LR = 5e-3  # initial learning rate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "\n",
    "\n",
    "# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)\n",
    "model = make_model()  # define our model\n",
    "\n",
    "# prepare model for fitting (loss, optimizer, etc)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # we train 2-way classification\n",
    "    optimizer=keras.optimizers.Adamax(lr=INIT_LR),  # for SGD\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")\n",
    "\n",
    "# scheduler of learning rate (decay with epochs)\n",
    "def lr_scheduler(epoch):\n",
    "    return INIT_LR * 0.9 ** epoch\n",
    "\n",
    "# callback for printing of actual learning rate used by optimizer\n",
    "class LrHistory(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        print(\"Learning rate:\", K.get_value(model.optimizer.lr))\n",
    "\n",
    "# fit model\n",
    "history = model.fit(\n",
    "    X_train, ytrain_2,  # prepared data\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), \n",
    "               LrHistory()],\n",
    "    validation_data=(X_test, y_test_2),\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.95433789954338%\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test).argmax(axis=-1)\n",
    "test_answers = y_test_2.argmax(axis=-1)\n",
    "test_accuracy = np.mean(test_predictions==test_answers)\n",
    "print(str(test_accuracy*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset 2 https://github.com/topazape/LSTM_Chem\n",
    "\n",
    "text_file = open(\"dataset_cleansed.smi\", \"r\")\n",
    "lines = text_file.read().splitlines()\n",
    "smiles2=lines\n",
    "\n",
    "valid_mols = []\n",
    "for smi in smiles2:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        valid_mols.append(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438540"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438552"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "*** Open Babel Warning  in WriteMolecule\n",
      "  No 2D or 3D coordinates exist. Stereochemical information will be stored using an Open Babel extension. To generate 2D or 3D coordinates instead use --gen2D or --gen3D.\n",
      "==============================\n",
      "*** Open Babel Warning  in ParseComplex\n",
      "  Illegal aromatic element te\n",
      "50000 molecules converted\n",
      "50001 warnings 63 info messages 1177020 audit log messages \n"
     ]
    }
   ],
   "source": [
    "#Taking first 50000 molecules of the dataset and converting them to sdf using OpenBabel\n",
    "with open('smiles.smi', 'w') as filehandle:\n",
    "    for listitem in smiles2[0:50000]:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "        \n",
    "!babel -ismi \"smiles.smi\" -osdf \"smiles.sdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Molecular Fingerprint(Morgan) from iwatobipen's website.\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import DataStructs\n",
    "\n",
    "\n",
    "mols = [mol for mol in Chem.SDMolSupplier('smiles.sdf') if mol != None]\n",
    "for mol in mols:\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "X = []\n",
    "for mol in mols:\n",
    "    arr = np.zeros((0,))\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    X.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce dimensions with TSNE and Tanimoto distance metric\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tanimoto_dist(a,b):\n",
    "    dotprod = np.dot(a,b)\n",
    "    tc = dotprod / (np.sum(a) + np.sum(b) - dotprod)\n",
    "    return 1.0-tc\n",
    "\n",
    "tsne = TSNE(n_components=2, metric=tanimoto_dist)\n",
    "tsne_X = tsne.fit_transform(X)\n",
    "print(tsne_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_X.T[0], tsne_X.T[1], color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarity search using K-nearest neighbors, use n=10 neighbors\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree',metric=tanimoto_dist).fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
